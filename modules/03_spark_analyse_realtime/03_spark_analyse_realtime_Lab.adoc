:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:

= Automated Analysis of Uber traffic using AMQ Streams

.Goals
. Importing of a json file of traffic cluster centers
. Consumption of raw Uber data record stream via a Kafka Topic
. Enrichment of streamed Uber data with assignment of specific traffic cluster center Id to each record
. Additional enrichment of streamed Uber data with assignment of a key performance indicator to each record
. Publishing of Enriched Data records to a different Kafka Topic for additional downstream rule processing
. Visualization of heat-map of simulated traffic in cluster centers


.Prerequisite
* Skills
** Programming knowledge using link:https://vertx.io/[vert.x]

:numbered:

== Overview

=== Key Performance Indicators

==== Adding Rank of the Cluster centers


=== Technical Background

. *link:https://spark.apache.org/docs/2.2.0/streaming-kafka-0-8-integration.html[Spark / Kafka Integration]*

. *link:https://kafka.apache.org/documentation/#api[Kafka Streaming API]*

.. The *Producer API* allows applications to send streams of data to topics in the Kafka cluster.
.. The *Consumer API* allows applications to read streams of data from topics in the Kafka cluster.
.. The *Streams API* allows transforming streams of data from input topics to output topics.
.. The *Connect API* allows implementing connectors that continually pull from some source system or application into Kafka or push from Kafka into some sink system or application.
.. The *AdminClient* API allows managing and inspecting topics, brokers, and other Kafka objects.

. *link:https://vertx.io/[vert.x]*


=== Architecture
. Data flow:
+
image::images/DataFlowDiagram.png[DataFlow]

.. Uber trip data is published to a Kafka Streams topic (uberrawdatatopic) using the Kafka API.
.. A Spark streaming application subscribes to the uberrawdatatopic and enriches the data with a traffic cluster Id
.. The Spark streaming application pushes the enriched data to a topic called: uberenricheddatatopic.
.. Ingests a stream of uber trip events Identifies the location cluster corresponding to the latitude and longitude of the uber trip Adds the cluster location to the event and publishes the results in JSON format to another topic A Spark streaming application subscribed to the second topic: Analyzes the uber trip location clusters that are popular by date and time.


. Deployment Topology diagram with DataFlow.
+
image::images/DeploymentTopologyLab3Data.png[DeploymentTopoData]

=== Modify the Environment Variables
In a previous lab, you should have already set various environment variables in the shell of your lab environment.

At this time, ensure that the following environment variables remain reset:

-----
echo "export OCP_PROJECT=\$OCP_USERNAME-uber-realtime-data" >> ~/.bashrc

echo "export OCP_REGION=`echo $HOSTNAME | cut -d'.' -f2`" >> ~/.bashrc

echo "export OCP_DOMAIN=clientvm.\$OCP_REGION.rhte.opentlc.com" >> ~/.bashrc

echo "export OCP_WILDCARD_DOMAIN=apps.\$OCP_DOMAIN" >> ~/.bashrc

source ~/.bashrc

$ echo $OCP_USERNAME
developer

$ echo $OCP_PASSWD
xxxxxxxx
-----

== Lab Asset Overview

This lab provides a set of assets to assist with the provisioning of Oshinko and Zeppelin.
You will want to clone these lab assets to your lab environment so that you can review them.

. Make a new directory where all lab assets will reside.
  Already the lab assets are cloned in Lab1. Please refer instructions of Lab1.
. Change directory to the previously cloned lab asset.
+
-----
$ cd $HOME/lab/operational_intelligence/uber-realtime-spark-stream-analysis

-----

. Review the various files specific to this lab :
+
-----
uber-realtime-spark-stream-analysis/
├── data
│   ├── cluster.txt
│   └── uber.csv
├ 
├── pom.xml
├── ReadMe.adoc
└── src
    └── main
        ├── java
        │   └── com
        │       └── streamskafka
        │           └── uber
        │               ├── MsgConsumer.java
        │               └── MsgProducer.java
        └── scala
            └── com
                ├── sparkkafka
                   └── uber
                       ├── UberEnrinchmentDataConsumer.Scala
                       |-- UberRawDataConsumer.Scala
                       |-- UberRawDataProducer.Scala

                |--redhat
                     |--gpte
                         |--UberData
                              |-- ClusterUber.scala

-----


. Several key assets to review are as follows:

.. *pom.xml*
+
Notice that community Apache Spark and community Scala packages are being utilized.
At this time, Red Hat does not intend to provide supported versions of these packages.

.. *Large Datasets of Uber Data*

... Available in the lab assets at:  `uber-data-analysis/src/main/resources/data/uber.csv`
... It is the raw data from the UberData for NLC which describes the Latitude, Longitude, timestamp and BaseId

.. *UberEnrinchmentDataConsumer.scala*

  ... Consumes the Uber.csv file which is a near-real-time-uber-data and send its to the UberTopic which has already been created in Lab1.
  ... Produces the Enriched UberData with KMeans clusterId which is send to the spark streaming which helps for accurate predictions.
 .. How Integration happening with Spark ?
 ... In Lab1 we create a topic called UberTopic which consumes the Uber.csv file and pushed into the spark-streaming. Please refer the deployment Topology diagram with Uber.csv file.


== Kafka Strimzi Assets
Recall the OC commands created in Lab1.


== Oshinko

=== Oshinko Web UI

. Log into OpenShift Environment using OC Client Tool to your Lab Region
+
-----
$ oc login https://$HOSTNAME:8443 -u $OCP_USERNAME -p $OCP_PASSWD
-----

. Create and switch to the OCP project specific to this lab:
+
-----
$ oc new-project $OCP_USERNAME-uber-realtime-data --description=$OCP_USERNAME-uber-realtime-data



$ oc project $OCP_USERNAME-uber-realtime-data
-----

. In your OpenShift namespace, create needed Oshinko templates:
+
-----
$ oc create \
     -f https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/1.0.3/templates/oshinko-cluster.yaml \
     -n $OCP_USERNAME-uber-realtime-data
-----

. Provision the Oshinko-WebUI
+
-----

$ oc new-app oshinko-webui -n $OCP_USERNAME-uber-realtime-data > /tmp/oshinko-web.txt

-----
+
.. Review the output found in /tmp/oshinko-web.txt
+
----
--> Deploying template "developer-uber-realtime-data/oshinko-webui" to project developer-uber-realtime-data

     * With parameters:
        * SPARK_DEFAULT=
        * OSHINKO_WEB_NAME=oshinko-web
        * OSHINKO_WEB_IMAGE=radanalyticsio/oshinko-webui:stable
        * OSHINKO_WEB_ROUTE_HOSTNAME=
        * OSHINKO_REFRESH_INTERVAL=5

--> Creating resources ...
    service "oshinko-web-proxy" created
    service "oshinko-web" created
    route "oshinko-web" created
    deploymentconfig "oshinko-web" created
--> Success
    Access your application via route 'oshinko-web-user3-uber-data.apps.6d13.openshift.opentlc.com'
    Run 'oc status' to view your app.

----
. Review the template that has been created
+
-----
$ oc get template oshinko-webui -n $OCP_USERNAME-uber-realtime-data -o yaml | more
-----


. Wait until both containers of the oshinko-web pod have started:
+
-----
$ oc get pods -w
NAME                  READY     STATUS    RESTARTS   AGE


oshinko-web-1-86blg   2/2       Running   0
-----


. Log into the Oshinko web UI
.. Point your browser to the output of the following command:
+
-----
$ echo -en "\n\nhttp://"$(oc get route/oshinko-web -o template --template {{.spec.host}} -n $OCP_USERNAME-uber-realtime-data)/webui"\n\n"
-----
+
image::images/oshinko_homepage.png[oshinko_homepage]

.. At this time, the Oshinko web UI is not secured. It is recommended to use Oshinko webui non-secured port.
+
Subsequently, you should be able to access the UI without authenticating and Ensure that OshinkWebUI is up and running. All our Modules will be deployed in the Oshinko cluster.


=== What we did in the Lab2?
In Lab2 we just created a model with the Historical data (Uber.csv), build a training set, Identified the patterns and did a Test Predictions.

In Lab3 we are going to use the Deployed Model and to give accurate predictions.

image::images/picture1.png[recall]

=== Importing of JSON File of different traffic cluster centers
.. In Lab2 we used the model with the Historical Data (uber.csv).
.. In Lab3 we are going to Enrich the data with ClusterId and the output format will be like as shown below.

image::images/uberEnricheddatatopic.png[ubet]

.. Visual representation of the data along with Cluster Centers will be displayed as shown below.

image::images/clusterCenters.png[cc]
.. We are saving this JSON File in our Version Control as cluster.txt file and we would use it for near Real Time Streaming.
.. uber.csv is our sample data which we used in our Lab2. This data is converted into cluster.txt file and it is done in our LocalIDE.

image::images/versioncontrol.png[vc]


==== How it is Handled?
It is handled in *LocalIDE* only, ClusterUber.scala code handles this.
----
//In this Line

model.write.overwrite().save("savemodel")
   // model can be  re-loaded like this
   // val sameModel = KMeansModel.load("/data/savemodel")
   //
   // to save the categories dataframe as json data
   val res = spark.sql("select dt, lat, lon, base, prediction as cid FROM uber order by dt")
   res.write.format("json").save("uber.json")
   res.write.format("txt").save("cluster.txt")
 }
----
.. uber.json file is persisted in a disk and it is stored in our GitHub.
.. It will be used for an Input for Spark Stream Processing.

=== Consumption of Raw Uber Data Record Stream Via a Kafka topic
.. Uber trip data is published to a UberRawData topic using the Kafka API.

image::images/uberrawdatatopic.png[UberRawData]

.. A Spark streaming application subscribed to the UberRawData topic.
Ingests a stream of uber trip events
... Identifies the location cluster corresponding to the latitude and longitude of the uber trip
... Adds the cluster location to the event and publishes the results in JSON format to uberenricheddatatopic
... A Spark streaming application subscribed to the uberEnricheddatatopic topic.
..... Analyzes the uber trip location clusters that are popular by date and time. Enriched Data will look like this below.
Addition of Cluster Center Parameter.

----
{"dt":"2014-08-01 00:04:00","lat":40.7047,"lon":-73.9349,"base":"B02617","cluster":6}
{"dt":"2014-08-01 00:06:00","lat":40.7226,"lon":-74.0034,"base":"B02598","cluster":9}
{"dt":"2014-08-01 00:06:00","lat":40.7577,"lon":-73.9619,"base":"B02617","cluster":3}
{"dt":"2014-08-01 00:06:00","lat":40.7489,"lon":-73.9777,"base":"B02617","cluster":8}
{"dt":"2014-08-01 00:06:00","lat":40.7672,"lon":-73.953,"base":"B02617","cluster":0}
----
=== Injecting the Data (JSON and Cluster.txt) in PVC.
. Create a configuration map based on the uber.csv data file found in your lab assets:

----
oc set volume dc/oshinko-web \
> --add --overwrite \
> --name=uber-data-volume \
> -t configmap \
> --configmap-name=uber-data-cm \
> -m /data/uber.csv \
> --sub-path=uber.csv \
> --default-mode=0644

----
. Create a configuration map based on the cluster.txt data file found in your lab assets:




=== Ranking - Why it is important?
.. In lab2, we could display the cluster centers in Zeppelin notebooks which shows the ranking. Here the ranking is static, which helps us to learn about the pattern recognition and historical analysis of data.
.. In lab3, we use the ranking which is dynamic helps in calculating the raise in price which is used by the Red Hat Decision Manager.

==== How it is handled?
.. In UberEnrinchmentDataConsumer.scala the following below lines indicate

----
val clust = categories.select($"dt", $"lat", $"lon", $"base", $"prediction".alias("cid")).orderBy($"dt")
 val res = clust.join(ccdf, Seq("cid")).orderBy($"dt")

 // Find the rank of Cluster using the parameters date, lat, lon and baseId and sort them.
 val rank= res.orderBy($"dt",$"lat",$"lon",$"base")
// val rank= clust.join(($"cid"),Seq("cid"))
 res.show
 rank.show
 //OutPut shown here with the cluster parameter in JSON file which is Rank
// {"dt":"2014-08-01 00:04:00","lat":40.7047,"lon":-73.9349,"base":"B02617","cluster":9}
// {"dt":"2014-08-01 00:06:00","lat":40.7226,"lon":-74.0034,"base":"B02598","cluster":6}
// {"dt":"2014-08-01 00:06:00","lat":40.7577,"lon":-73.9619,"base":"B02617","cluster":3}
// {"dt":"2014-08-01 00:06:00","lat":40.7489,"lon":-73.9777,"base":"B02617","cluster":2}
// {"dt":"2014-08-01 00:06:00","lat":40.7672,"lon":-73.953,"base":"B02617","cluster":0}

----

=== Enrichment of UberRawData
==== Why Data Enrichment is needed?
... In-order to determine on how many pickups occurred in each cluster.
... Which Hours of the day and which cluster had the highest number of pickups.
... In-order to identify the Heatmap Layer for Realtime Dashboard Display.

==== Deployment Methodology

===== Execution of ClusterUber Application on Oshinko CLUSTER
Via the OC Command Utility we can deploy this Module using the below oc command line.

.. The Command Line Arguments is given below :
+
-----
$ oc new-app --template oshinko-java-spark-build-dc \
    -p APPLICATION_NAME=uber-real-time-analysis \
    -p APP_MAIN_CLASS=com.redhat.gpte.uberdata.ClusterUber \
    -p GIT_URI= https://github.com/gpe-mw-training/operational_intelligence.git \
    -p APP_FILE=uber-spark-stream-analysis.jar
-----

.. Check the Build logs
-----
oc logs -f bc/uber-spark-stream-analysis >>bcuber-spark.txt
-----

.. Check the Deployment logs
-----
oc logs -f dc/uber-spark-stream-analysis >>dcuber-spark.txt
-----

== Visualization of Heat-Map of simulated Traffic in Cluster centers
=== The vert.x toolkit and Web Application Architecture
The Vert.x toolkit is event-driven, using an event bus to distribute events to work handler services, called verticles. Vert.x, similar to Node.js, employs a non-blocking model with a single threaded event-loop to handle work. The Vert.x SockJS event bus bridge allows web applications to communicate bi-directionally with the Vert.x event bus using Websockets, which allows you to build real-time web applications with server push functionality.

image::images/Vert.xGeneral.png[Vertx]

==== Deployment diagram
.. A Vert.x Kafka client verticle consumes messages from the Kafka Streams topic and publishes the messages on a Vert.x event bus.
.. A Javascript browser client subscribes to the Vert.x event bus using SockJS and displays the Uber trip locations on a Google Heatmap.

image::images/VertxDataFlow.png[vertxdataflow]

==== Deployment in OpenShift.
. Create a build using the following command.
----
$ oc create -f https://raw.githubusercontent.com/gpe-mw-training/operational_intelligence/master/templates/vertx.yaml

OutPut

buildconfig "vertx-s2i" created
imagestream "vertx-centos" created
imagestream "vertx-s2i" created
template "uber-heatmap" created

$ oc new-app uber-heatmap

OutPut

--> Deploying template "developer-uber-realtime-data/uber-heatmap" to project developer-uber-realtime-data

     uber-heatmap
     ---------
     RealTime Vert.x application build with Maven

     * With parameters:
        * APPLICATION_NAME=uber-heat-map-analysis
        * APPLICATION_HOSTNAME=
        * GIT_URI=https://github.com/gpe-mw-training/operational_intelligence.git
        * GIT_REF=initial-work
        * CONTEXT_DIR=uber-heat-map-analysis
        * APP_OPTIONS=
        * GITHUB_TRIGGER_SECRET=gsnhp1Lu # generated
        * GENERIC_TRIGGER_SECRET=LEvXMMcl # generated

--> Creating resources ...
    buildconfig "uber-heat-map-analysis" created
    imagestream "uber-heat-map-analysis" created
    deploymentconfig "uber-heat-map-analysis" created
    route "uber-heat-map-analysis" created
    service "uber-heat-map-analysis" created
--> Success
    Build scheduled, use 'oc logs -f bc/uber-heat-map-analysis' to track its progress.
    Access your application via route 'uber-heat-map-analysis-developer-uber-realtime-data.apps.clientvm.1f6b.openshift.opentlc.com'
    Run 'oc status' to view your app.

----


==== OutPut Visualization representation

*The Dashboard Vert.x HTML5 Javascript Client*
The client uses a Google Maps Heatmap Layer to visually depict the intensity of the Uber trip cluster locations on a Manhattan Google map. With the Google Heatmap, areas of higher intensity will be colored red, and areas of lower intensity will appear green. The dashboard app uses Google Maps markers to mark cluster centers.

image::images/Vertx_realtime.png[vr]
.. The messages received from the server application are in JSON format and contain the following for each trip location: the cluster center id, datetime, latitude and longitude for the trip, base for the trip, and latitude and longitude for the cluster center.

An example is shown below:

----
{"cid":18, "dt":"2014-08-01 08:51:00", "lat":40.6858, "lon":-73.9923, "base":"B02682", "clat":40.67462874550765, "clon":-73.98667466026531}
----



== Conclusions

====  What did we learn?

Apache Strimzi - Basics of Apache Strimzi and it's deployment on OpenShift.

Spark Streaming - Excellent API for structured streaming and it is an advanced concept in Apache Spark. Since, it uses catalyst optimizer, it provides an excellent performance benefits and it is the most prefered query language for the datascientists all over the world.

Kafka with Apache Spark Integration - We learned Apache Kafka integration with Spark on Strimzi Cluster.

== Questions

TO-DO :  questions to test student knowledge of the concepts / learning objectives of this lab

== Appendix

=== Where do we get these DataSets?

http://data.beta.nyc/dataset/uber-trip-data-foiled-apr-sep-2014

ifdef::showscript[]

=== ClusterQuota and Limit Range for Zeppelin Interpreter

==== Cluster Quota
A resource quota, defined by a ResourceQuota object, provides constraints that limit aggregate resource consumption per project. It can limit the quantity of objects that can be created in a project by type, as well as the total amount of compute resources and storage that may be consumed by resources in that project.

==== Limit Range
A limit range, defined by a LimitRange object, enumerates compute resource constraints in a project at the pod, container, image, image stream, and persistent volume claim level, and specifies the amount of resources that a pod, container, image, image stream, or persistent volume claim can consume.

All resource create and modification requests are evaluated against each LimitRange object in the project. If the resource violates any of the enumerated constraints, then the resource is rejected. If the resource does not set an explicit value, and if the constraint supports a default value, then the default value is applied to the resource.

By default, all OCP projects are assigned a limit range.  the limit range assigns default limits and requests for both CPU and RAM if the DCs themselves don't specify limits and requests.
The default CPU limit is set to 1/20th of a CPU.  So Spark was running on 1/20th of a CPU.

In general, all of us should always understand the details of LimitRanges assigned to our projects.
And its very likely that we should be adding/tweaking the limits and requests in our DC's.

==== CPU Limits

Each container in a pod can specify the amount of CPU it is limited to use on a node. CPU limits control the maximum amount of CPU that your container may use independent of contention on the node. If a container attempts to exceed the specified limit, the system will throttle the container. This allows the container to have a consistent level of service independent of the number of pods scheduled to the node.

==== Memory Requests
By default, a container is able to consume as much memory on the node as possible. In order to improve placement of pods in the cluster, specify the amount of memory required for a container to run. The scheduler will then take available node memory capacity into account prior to binding your pod to a node. A container is still able to consume as much memory on the node as possible even when specifying a request.

==== Memory Limits
If you specify a memory limit, you can constrain the amount of memory the container can use. For example, if you specify a limit of 200Mi, a container will be limited to using that amount of memory on the node. If the container exceeds the specified memory limit, it will be terminated and potentially restarted dependent upon the container restart policy.

=== Do we need to Know them
The above parameters are managed by the cluster Administrator and Infrastructure team, Hence it is not needed for the students to learn. But a basic concept of Knowing this will help.

*Students are expected to learn this much alone.*
----
For Viewing Quotas

$ oc get quota -n user3-uber-data
NAME                AGE
besteffort          11m
compute-resources   2m
object-counts       29m
...
...
$ oc describe quota object-counts -n user3-uber-data
Name:			object-counts
Namespace:		user3-uber-data
Resource		Used	Hard
--------		----	----
configmaps		3	10
persistentvolumeclaims	0	4
replicationcontrollers	3	20
secrets			9	10
services		2	10

For Viewing Limit Ranges

$ oc get limits -n user3-uber-data
NAME              AGE
resource-limits   6d

$ oc describe limits resource-limits
Name:		resource-limits
Namespace:	use3-uber-data
Type		Resource	Min	Max	Default Request	Default Limit	Max Limit/Request Ratio
----		--------	---	---	---------------	-------------	-----------------------
Pod		cpu		30m	2	-		-		-
Pod		memory		150Mi	1Gi	-		-		-
Container	memory		150Mi	1Gi	307Mi		512Mi		-
Container	cpu		30m	2	60m		1		-

$ oc describe limits resource-limits -n user3-uber-data
Name:                           resource-limits
Namespace:                      demoproject
Type                            Resource                Min     Max     Default Request Default Limit   Max Limit/Request Ratio
----                            --------                ---     ---     --------------- -------------   -----------------------
Pod                             cpu                     200m    2       -               -               -
Pod                             memory                  6Mi     1Gi     -               -               -
Container                       cpu                     100m    2       200m            300m            10
Container                       memory                  4Mi     1Gi     100Mi           200Mi           -
openshift.io/Image              storage                 -       1Gi     -               -               -
openshift.io/ImageStream        openshift.io/image      -       12      -               -               -
openshift.io/ImageStream        openshift.io/image-tags -       10      -               -               -


endif::showscript[]
